{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yX2V-RTlOCMp"
   },
   "source": [
    "# Meet your Artificial Self\n",
    "\n",
    "> This notebook will be used for the [AMLD 2020](https://appliedmldays.org/) workshop [**\"Meet your Artificial Self\"**](https://appliedmldays.org/workshops/meet-your-artificial-self-generate-text-that-sounds-like-you), taking place January 25 in Lausanne, Switzerland.\n",
    "\n",
    "## Task 2\n",
    "In task 1 we learnt how to fine-tune a language model and we saw how style transfer works. Conversations are a different beast however! \n",
    "\n",
    "In this task we will try our first approach at training a conversational model.\n",
    "\n",
    "## Important resources\n",
    "* [Workshop Github repo](https://github.com/mar-muel/artificial-self-AMLD-2020/tree/master/2)\n",
    "* [PyTorch documentation](https://pytorch.org/docs/stable/index.html)\n",
    "* Huggingface transformers library [ [Github](https://github.com/huggingface/transformers) | [Docs](https://huggingface.co/transformers/) ]\n",
    "\n",
    "## Approach\n",
    "In this task we will try a naive approach to getting conversational style by simply feeding the model \"raw\" conversation data of the form:\n",
    "```\n",
    "<speaker1> Hi\n",
    "<speaker2> Hey - how are you?\n",
    "<speaker1> Great, thanks!\n",
    "...\n",
    "```\n",
    "Our hope is that the model will simply learn this structure and we will be able to query the model with an input of the form:\n",
    "\n",
    "```\n",
    "<speaker2> Am I speaking to a bot?\n",
    "<speaker1>\n",
    "```\n",
    "We then expect the model to extend the text from this prefix.\n",
    "\n",
    "This notebook will run you through all the steps from collecting the training data until interacting with the final model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUyajaGkR1JM"
   },
   "source": [
    "# Setting things up\n",
    "The following cells will clone the repository and install all the necessary dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2e-ZKZISylw"
   },
   "outputs": [],
   "source": [
    "# Install all dependencies for this task\n",
    "!pip3 install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pM82B4aUXg8"
   },
   "source": [
    "# Import training data\n",
    "In this step we will add the data to Colab to train the model. You are free to choose from two options:\n",
    "2. Use conversational data from well known people (\"Alternative data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAE4MrVSVmV1"
   },
   "source": [
    "# Alternative dataset 1: world leader interviews\n",
    "\n",
    "Instead of using your own data, you can use some datasets we prepared for you. The first option is a dataset of interviews of world leaders: Barack Obama and Vladimir Putin. These interviews will be treated as chat conversations, where the interlocutors are the reporters.\n",
    "\n",
    "To use those, copy the conversation you want from the `datasets` folder to the task2 `data` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9PmICPoWt98"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-318978a110de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#input_data = '../datasets/vladimir_putin_interviews.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"File not found\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Barack Obama\n",
    "input_data = './data/barack_obama_interviews2.json'\n",
    "\n",
    "# Vladimir Putin\n",
    "#input_data = '../datasets/vladimir_putin_interviews.json'\n",
    "\n",
    "assert os.path.isfile(input_data), \"File not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQCXoLLhO2FG"
   },
   "source": [
    "# Alternative dataset 2: movie quotes\n",
    "\n",
    "Another option is to use quotes from movies. You can use the [Cornell Movie-Dialogs Corpus](https://), which contains **220,579 conversational exchanges** between 10,292 pairs of movie characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRYuNd85O5cl"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7b36ccd44761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#input_data = '../datasets/cornell_movie_dialogs_corpus.json.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"File not found\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#input_data = '../datasets/cornell_movie_dialogs_corpus.json.zip'\n",
    "\n",
    "assert os.path.isfile(input_data), \"File not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxNG4fmqcHGt"
   },
   "source": [
    "# Prepare the data\n",
    "For this task we will use the transfomers library by Huggingface. The transformers library implements many recent NLP models (such as BERT and GPT-2).\n",
    "\n",
    "Our data is currently in JSON format, but can easily be read into a Pandas Dataframe.\n",
    "\n",
    "As a first step we want to get our conversation data from this format:\n",
    "\n",
    "| timestamp | conversationId | conversationWithName | senderName | outgoing | text | language | platform |\n",
    "| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n",
    "| 1575463019 | 693342290 | Alice | Bob | True | Hi Alice! | en | whatsapp |\n",
    "| 1575463025 | 693342290 | Alice | Alice | False | Hi Bob! How are you these days? | en | whatsapp |\n",
    "| 1575463030 | 693342290 | Alice | Bob | True | Great! Thanks | en | whatsapp |\n",
    "\n",
    "and get into a text file of this format:\n",
    "```\n",
    "<person1> Hi Alice!\n",
    "<person2> Hi Bob! How are you these days?\n",
    "<person1> Great! Thanks\n",
    "...\n",
    "```\n",
    "\n",
    "Our model will try to generate this structure from the data. The tags `<person1>` and `<person2>` have nothing special to them and could in theory be replaced by something else as well (more about this below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MLHMIoUbTFp-"
   },
   "outputs": [],
   "source": [
    "from utils import generate_input_task2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bvdcn7HBTGgG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 166.46it/s]\n"
     ]
    }
   ],
   "source": [
    "assert os.path.isfile(input_data)\n",
    "generate_input_task2(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00KS5e8KYpWz"
   },
   "source": [
    "The script has now generated an input file `cached_input_task2.txt`. You can inspect it with the `head` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUh1_4LBpA05",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<speaker2> Mr President, you're about to fly to Kenya, to your ancestral home. Given the al-Shabaab attacks on the West Gate mall and Garissa University, I'm sure your secret service could've suggested other countries for you to visit. But you wanted to go to Kenya. Well, I think it is important first of all that the president of the United States underscores our commitment to partnering with countries around the world, even though we're not intimidated by terrorist organisations. Second, the counterterrorism co-operation between the United States and Kenya - and Uganda and other countries - in East Africa - is very strong. And part of the subject of the visit is to continue to strengthen those ties to make them more effective. Third, as I wind down my presidency, I've already had a number of visits to Africa. But this gives me an opportunity to focus on a region that I have not been visiting as president, and I'm also going to have the opportunity to talk to the African Union. So I'll be the first US president to not only visit Kenya and Ethiopia, but also to address the continent as a whole, building off the African summit that we did here which was historic and has, I think, deepened the kinds of already strong relationships that we have across the continent. And you're going to talk about entrepreneurship at this summit in Nairobi.\r\n",
      "<speaker1> Uh-huh.\r\n",
      "<speaker2> Is there any link between security and entrepreneurship?\r\n",
      "<speaker1> I think there is. I believe that when people see opportunity, when they have a sense of control of their own destiny, then they're less vulnerable to the propaganda and twisted ideologies that have been attracting young people - particularly now being turbocharged through social media. And a while back, when we started looking at strategies to reach out to the Muslim world - to reach out to - developed countries, a common theme emerged, which was people are not interested in - just being patrons- or - or being patronised. And being given aid. They're interested in building capacity. The more we can encourage entrepreneurship, particularly for young people, the more they have hope. Now that requires some reforms in these governments that we continue to emphasise. Rooting out corruption, increased transparency and how government operates, making sure that regulations are not designed just to advantage elites, but are allowing people who have a good idea to get out there and get things done.\r\n",
      "<speaker2> And I suppose the - you know, you famously said when you went to Africa, I think when you first became president, you know, \"What we need is strong institutions and- \"\r\n",
      "<speaker1> Yes.\r\n",
      "<speaker2> \" - not strong men\". You're going to Ethiopia, where there is effectively no opposition in Parliament.\r\n",
      "<speaker1> Right.\r\n",
      "<speaker2> You're going to Kenya, where the International Criminal Court is still investigating certain members of the government, which seems kind of hardly ideal institutions.\r\n",
      "<speaker1> Well, they're not ideal institutions. But what we found is, is that when we combined blunt talk with engagement, that gives us the best opportunity to influence and open up space for civil society. And the human rights agenda that we think is so important. And, you know, a good example of this is Burma. Where I was the first US president to visit there. At a time when we saw some possibility of transition, by the time I landed in Burma - it is not a liberal democracy by any means. And there were still significant human rights violations taking place. But my visit then solidified and validated the work of dissenters and human rights activists. And that has continued to allow them to move in the direction of a democracy. So, so our view is, in the same way that I visited Russia, and in the same way that I visited China, even when we know that there are significant human rights violations taking place, we want to make sure that we're there so that we can have this conversation and point them in a better direction.\r\n"
     ]
    }
   ],
   "source": [
    "!head cached_input_task2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9gLQPF6Y-K4"
   },
   "source": [
    "# Train the model\n",
    "We can now start training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2GsvtlOlZSAF"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    OpenAIGPTConfig,\n",
    "    OpenAIGPTLMHeadModel,\n",
    "    OpenAIGPTTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from utils import get_input_task2, set_seed, add_special_tokens_\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s [%(levelname)-5.5s] [%(name)-12.12s]: %(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYeG8QI0ce_W"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckAH0GK5aH0t"
   },
   "outputs": [],
   "source": [
    "run_name = 'run1'               # The name of the run (subdirectory in ./runs)\n",
    "model_type = 'openai-gpt'       # Initialize model from path to checkpoint or with model name (\"openai-gpt\" or \"gpt2\")\n",
    "save_every = 50                 # Save checkpoint every n updates steps.\n",
    "max_input_length = 400          # Number of tokens which will be fed into the model (reduce this number if you have memory constraints)\n",
    "weight_decay = 0                # Weight decay if we apply some.\n",
    "train_batch_size = 8            # Batch size for training\n",
    "gradient_accumulation_steps = 8 # Accumulate gradients on several steps\n",
    "lr = 5e-5                       # Learning rate\n",
    "adam_epsilon = 1e-8             # Epsilon for Adam optimizer.\n",
    "max_norm = 1                    # Clipping gradient norm\n",
    "n_epochs = 2                    # Number of training epochs\n",
    "device = 'cpu'                 # Device (cuda or cpu)\n",
    "warmup_steps = 0                # Linear warmup over warmup_steps.\n",
    "seed = 42                       # random seed for initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFnOq8n4cib3"
   },
   "source": [
    "### Data loading\n",
    "In PyTorch we can define a class which inherits from the Dataset class containing an initializer method `__init__()` in which we \n",
    "\n",
    "1. Read the text file which we generated before\n",
    "2. Tokenize the text (split it the text into smaller words/character pairs) and convert the tokens into vocabulary IDs (the positions of the tokens in the vocabulary. GPT-2 uses so-called BPE (byte-pair encoding). If you're interested how it works you can read more about it in [this blog post](https://leimao.github.io/blog/Byte-Pair-Encoding/).\n",
    "3. Cut up the array of token IDs into chunks of size `max_input_length` (usually chosen to the maximum of what the memory/model allows for)\n",
    "4. Append the generated training example as a list\n",
    "\n",
    "The `__get_item__()` simply implements the retrieval of a new training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xeBZosNKZYUa"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer):\n",
    "        # load the text data generated from before into memory\n",
    "        text = get_input_task2(input_data)\n",
    "        logger.info(\"Tokenizing and building input...\")\n",
    "        # tokenize the whole file\n",
    "        tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "        # generate training examples by cutting the text into blocks of size max_input_length\n",
    "        self.examples = []\n",
    "        block_size = max_input_length\n",
    "        if block_size < 0:\n",
    "            # use maximum possible input block size\n",
    "            block_size = tokenizer.max_len_single_sentence\n",
    "        for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
    "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
    "            self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size]))\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item])\n",
    "\n",
    "def get_data_loader(tokenizer):\n",
    "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
    "    dataset = TextDataset(tokenizer)\n",
    "    logger.info(\"Train dataset: {:,} samples\".format(len(dataset)))\n",
    "    logger.info(\"Build dataloaders\")\n",
    "    data_loader = DataLoader(dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7voPNvpgjL2"
   },
   "outputs": [],
   "source": [
    "# Setting the same seed allows for some reproducibility of the experiments\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXbo_iLqiEjf"
   },
   "source": [
    "### Load models and tokenizers\n",
    "The transformers library comes with a built in method `from_pretrained(model_type)`. `model_type` can specify either\n",
    "1. One of the [pretrained model architectures](https://huggingface.co/transformers/pretrained_models.html). In this case the model will be downloaded and cached on disk before loaded into memory.\n",
    "2. The path to a folder with an existing model checkpoint.\n",
    "\n",
    "This allows us to use the same syntax to either pretrained or fine-tuned models/tokenizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QddFgKLlZlHI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-12 21:51:35,095 [INFO ] [__main__    ]: Prepare tokenizer, pretrained model and optimizer.\n",
      "2020-07-12 21:51:35,104 [DEBUG] [urllib3.conn]: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2020-07-12 21:51:35,621 [DEBUG] [urllib3.conn]: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/openai-gpt-vocab.json HTTP/1.1\" 200 0\n",
      "2020-07-12 21:51:35,638 [DEBUG] [urllib3.conn]: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2020-07-12 21:51:36,235 [DEBUG] [urllib3.conn]: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/openai-gpt-merges.txt HTTP/1.1\" 200 0\n",
      "2020-07-12 21:51:36,242 [INFO ] [transformers]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json from cache at /home/benjamin/.cache/torch/transformers/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
      "2020-07-12 21:51:36,246 [INFO ] [transformers]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt from cache at /home/benjamin/.cache/torch/transformers/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
      "2020-07-12 21:51:36,250 [WARNI] [transformers]: ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "2020-07-12 21:51:36,329 [DEBUG] [urllib3.conn]: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2020-07-12 21:51:36,951 [DEBUG] [urllib3.conn]: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/openai-gpt-config.json HTTP/1.1\" 200 0\n",
      "2020-07-12 21:51:36,955 [INFO ] [transformers]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json from cache at /home/benjamin/.cache/torch/transformers/a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.bd0797be126548711309ad2174d2afb16e3c37e891707667603d85e35a4ad001\n",
      "2020-07-12 21:51:36,956 [INFO ] [transformers]: Model config {\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"openai-gpt\",\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "2020-07-12 21:51:36,959 [DEBUG] [urllib3.conn]: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2020-07-12 21:51:37,565 [DEBUG] [urllib3.conn]: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/openai-gpt-pytorch_model.bin HTTP/1.1\" 200 0\n",
      "2020-07-12 21:51:37,568 [INFO ] [transformers]: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin from cache at /home/benjamin/.cache/torch/transformers/e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIGPTLMHeadModel(\n",
       "  (transformer): OpenAIGPTModel(\n",
       "    (tokens_embed): Embedding(40478, 768)\n",
       "    (positions_embed): Embedding(512, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=40478, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Load tokenizer\n",
    " logger.info(\"Prepare tokenizer, pretrained model and optimizer.\")\n",
    " tokenizer_class = GPT2Tokenizer if \"gpt2\" in model_type else OpenAIGPTTokenizer\n",
    " tokenizer = tokenizer_class.from_pretrained(model_type)\n",
    " # Load model\n",
    " model_class = GPT2LMHeadModel if \"gpt2\" in model_type else OpenAIGPTLMHeadModel\n",
    " model = model_class.from_pretrained(model_type)\n",
    " model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xsD0MYlajkFE"
   },
   "source": [
    "### Add special tokens\n",
    "Let's see how `<speaker1>` and `<speaker2>` tokens will be tokenized by our current tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9s0G97YYjz1D"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<</w>', 'speaker', '1</w>', '></w>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('<speaker1>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7YQdj8ikKKA"
   },
   "source": [
    "As you can see the model generates a total of 4 tokens: `['<', 'speaker', '1', '>']`. This doesn't make too much sense for us since the tags should not contain any meaning but should simply indicate who is currently speaking. Luckily there is an easy way to add our speaker tokens to the vocabulary of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o11KdvWNjWik",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-12 21:51:41,641 [INFO ] [transformers]: Adding <speaker1> to the vocabulary\n",
      "2020-07-12 21:51:41,643 [INFO ] [transformers]: Adding <speaker2> to the vocabulary\n",
      "2020-07-12 21:51:41,643 [INFO ] [transformers]: Assigning ('<speaker1>', '<speaker2>') to the additional_special_tokens key of the tokenizer\n"
     ]
    }
   ],
   "source": [
    "ATTR_TO_SPECIAL_TOKEN = {'additional_special_tokens': ('<speaker1>', '<speaker2>')}\n",
    "# Add special tokens if they are not already added\n",
    "def add_special_tokens_(model, tokenizer):\n",
    "   \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"                                                                                   \n",
    "   orig_num_tokens = len(tokenizer.encoder)\n",
    "   num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there                                                                   \n",
    "   if num_added_tokens > 0:\n",
    "       model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
    "       \n",
    "add_special_tokens_(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJOfCIAblBH1"
   },
   "source": [
    "Now the model should generate a single token:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk0ehp_KlDYn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<speaker1>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('<speaker1>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r7hnYlQklwmE"
   },
   "source": [
    "# Final setup before training\n",
    "We need to set up a few things before we can start training:\n",
    "* Prepare the data loaders (discussed above)\n",
    "* An optimizer (we will use Adam)\n",
    "* A scheduler to change the learning rate throughout training (we will use a [linear schedule with warmup](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdj6wabyjQ0V",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-12 21:51:42,259 [INFO ] [__main__    ]: Prepare datasets\n",
      "2020-07-12 21:51:42,260 [INFO ] [/home/benjam]: Reading cached input file from cached_input_task2.txt...\n",
      "2020-07-12 21:51:42,262 [INFO ] [__main__    ]: Tokenizing and building input...\n",
      "2020-07-12 21:51:42,285 [INFO ] [__main__    ]: Train dataset: 2 samples\n",
      "2020-07-12 21:51:42,286 [INFO ] [__main__    ]: Build dataloaders\n"
     ]
    }
   ],
   "source": [
    " # Get data loaders\n",
    " logger.info(\"Prepare datasets\")\n",
    " data_loader = get_data_loader(tokenizer)\n",
    " # Prepare optimizer and schedule (linear warmup and decay)\n",
    " no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    " optimizer_grouped_parameters = [\n",
    "     {\n",
    "         \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": weight_decay,\n",
    "     },\n",
    "     {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    " ]\n",
    " optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=adam_epsilon)\n",
    " t_total = len(data_loader) // gradient_accumulation_steps * n_epochs\n",
    " scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your training time is approx. 0 min\n"
     ]
    }
   ],
   "source": [
    "print(f\"Your training time is approx. {len(data_loader)*n_epochs/((8/train_batch_size)*60):.0f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TMwH4HnnLGw"
   },
   "source": [
    "### Training\n",
    "\n",
    "Finally we can start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dOu849kknPnz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-12 21:51:42,321 [INFO ] [__main__    ]: ***** Running training *****\n",
      "Average loss: 4.2099: 100%|██████████| 1/1 [00:03<00:00,  3.81s/it]\n",
      "Average loss: 4.1576: 100%|██████████| 1/1 [00:03<00:00,  3.92s/it]\n",
      "Epoch [2/2]: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]\n",
      "2020-07-12 21:51:50,063 [INFO ] [__main__    ]: Saving model checkpoint to runs/run1\n",
      "2020-07-12 21:51:50,065 [INFO ] [transformers]: Configuration saved in runs/run1/config.json\n",
      "2020-07-12 21:51:50,618 [INFO ] [transformers]: Model weights saved in runs/run1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('runs/run1/vocab.json',\n",
       " 'runs/run1/merges.txt',\n",
       " 'runs/run1/special_tokens_map.json',\n",
       " 'runs/run1/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "global_step = 0\n",
    "epochs_trained = 0\n",
    "steps_trained_in_current_epoch = 0\n",
    "# Check if we are training from a checkpoint or from a pretrained model\n",
    "if os.path.exists(model_type):\n",
    "    # set global_step to gobal_step of last saved checkpoint from model path\n",
    "    global_step = int(model_type.split(\"-\")[-1].split(\"/\")[0])\n",
    "    epochs_trained = global_step // (len(data_loader) // gradient_accumulation_steps)\n",
    "    steps_trained_in_current_epoch = global_step % (len(data_loader) // gradient_accumulation_steps)\n",
    "    logger.info(\"Continuing training from checkpoint, will skip to saved global_step\")\n",
    "    logger.info(f\"Continuing training from epoch {epochs_trained}\")\n",
    "    logger.info(f\"Continuing training from global step {global_step}\")\n",
    "    logger.info(f\"Will skip the first {steps_trained_in_current_epoch} steps in the first epoch\")\n",
    "\n",
    "# Training loop\n",
    "model.zero_grad()\n",
    "epoch_pbar = trange(epochs_trained, int(n_epochs)) # epoch progress bar\n",
    "av_loss = 0\n",
    "for current_epoch in epoch_pbar:\n",
    "    epoch_pbar.set_description(f\"Epoch [{current_epoch+1}/{n_epochs}]\") # description of epoch progress bar\n",
    "    pbar = tqdm(data_loader, position=0) # progress bar\n",
    "    for step, batch in enumerate(pbar):\n",
    "        # Skip past any already trained steps if resuming training\n",
    "        if steps_trained_in_current_epoch > 0:\n",
    "            steps_trained_in_current_epoch -= 1\n",
    "            continue\n",
    "        model.train()\n",
    "        # the language model targets (labels) are the same as the input!\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        loss, *_ = model(inputs, labels=labels)\n",
    "        loss.backward()\n",
    "        tr_loss = loss.item()\n",
    "        # Compute a running average of the loss\n",
    "        av_loss = (step*av_loss + tr_loss)/(step + 1)\n",
    "        pbar.set_description(f\"Average loss: {av_loss:.4f}\")\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "            if global_step % save_every == 0 and global_step > 0:\n",
    "                checkpoint_prefix = \"checkpoint\"\n",
    "                output_dir = os.path.join('runs', run_name, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                logger.info(f\"Saving model checkpoint to {output_dir}\")\n",
    "                model.save_pretrained(output_dir)\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                logger.info(f\"Saving optimizer and scheduler states to {output_dir}\")\n",
    "                torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "\n",
    "# save model\n",
    "output_dir = os.path.join('runs', run_name)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "logger.info(f\"Saving model checkpoint to {output_dir}\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Ew8SGqBB-Te"
   },
   "source": [
    "# Tweaking parameters\n",
    "\n",
    "You can change the training parameters to see how they affect the language model: adjust them below, then run the again the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRyEvCxPB_Np"
   },
   "outputs": [],
   "source": [
    "run_name = 'run1'               # The name of the run (subdirectory in ./runs)\")\n",
    "model_type = 'openai-gpt'       # Initialize model from path to checkpoint or with model name (openai-gpt/openai-gpt2)\"\n",
    "weight_decay = 0                # Weight decay if we apply some.\n",
    "train_batch_size = 4            # Batch size for training\n",
    "gradient_accumulation_steps = 8 # Accumulate gradients on several steps\n",
    "lr = 5e-5                       # Learning rate\n",
    "n_epochs = 1                    # Number of training epochs\n",
    "warmup_steps = 0                # Linear warmup over warmup_steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_v4Y2p3RpOBB"
   },
   "source": [
    "# Interact with the model\n",
    "The trained model can now be found under `./runs/{run_name}/`.\n",
    "\n",
    "Let's see what happens when we feed in some text to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiT2mrSAqvWS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:\n",
      "tensor([[3570, 1276]])\n",
      "Model output:\n",
      "tensor([[[-6.2202e+00, -4.9445e+00, -1.6549e+01,  ...,  8.3677e-01,\n",
      "           1.5425e-02,  2.1317e-01],\n",
      "         [-6.6601e+00, -4.8935e+00, -1.8000e+01,  ...,  8.2694e-01,\n",
      "          -1.0044e-01,  2.3388e-01]]], grad_fn=<UnsafeViewBackward>)\n",
      "Output shape:\n",
      "torch.Size([1, 2, 40480])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode(\"Hello world\", add_special_tokens=True), device=device).unsqueeze(0)\n",
    "print('Input IDs:')\n",
    "print(input_ids)\n",
    "out, = model(input_ids)\n",
    "print('Model output:')\n",
    "print(out)\n",
    "print('Output shape:')\n",
    "print(out.shape) # output shape: (batch size x sequence length x hidden size)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CuP7ua6Jt_WO"
   },
   "source": [
    "As you can see the output of the model outputs a tensor of the size of the number of input tokens. By getting the highest value of the last dimension of the output tensor we can get the most likely next token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xO7VgD0sqxzu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([torch.argmax(out[:, 1, :])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuHx8BTrvVsW"
   },
   "source": [
    "## Start chatting\n",
    "As we have seen in task 1 we have several hyperparameters to choose from in order to control how we sample from the output probability distribution. Just choosing always the most likely token will not lead to interesting conversations.\n",
    "\n",
    "Below you see how the different sampling strategies are implemented. By default we will use top-p sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w5UrWZw6u1M7"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "max_history = 2                  # Number of previous utterances to keep in history\n",
    "no_sample = False                # Set to use greedy decoding instead of sampling\n",
    "max_length = 80                  # Maximum length of the output utterances\n",
    "temperature = 1.0                # Sampling softmax temperature\n",
    "top_k = 0                        # Filter top-k tokens before sampling (<=0: no filtering)\n",
    "top_p = 0.8                      # Nucleus filtering (top-p) before sampling (<=0.0: no filtering)\n",
    "no_info = False                   # Only show conversation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QaL732AIxCYl"
   },
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "def sample_sequence(conversation, model, num_samples=1):\n",
    "    \"\"\"Generate next tokens from pervious conversation\"\"\"\n",
    "    context = torch.tensor(conversation, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    generated = context\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            inputs = {'input_ids': generated}\n",
    "            outputs = model(**inputs)\n",
    "            # scale by temperature\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.) \n",
    "            # filter by top-k/top-p\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            if temperature == 0: # greedy sampling:\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBpALwQAyWb9"
   },
   "source": [
    "We are ready to interact with the model. A few things to note:\n",
    "* We will give it a \"trigger\" to start the conversation. From there the model takes it. Note that the model is \"playing\" speaker1 and you are speaker2. \n",
    "* If the `no_info` flag is set to `False`, the output shows both the input (conversation history) as well as the full output of the model. At the very end the answer which was selected by the model is shown.\n",
    "* You can press `h` (and then ENTER) in order to see the whole history of the chat\n",
    "\n",
    "\n",
    "Enjoy! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4gwwMEoxwAd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<speaker2> Hi!\n",
      "<speaker1> Hello\n",
      "<speaker2> Are you ready?\n",
      "<speaker1> Yes!\n",
      "<speaker2> Ok let's start chatting\n",
      "<speaker1> Sure, what do you want to talk about?\n",
      "\n",
      "[Chat with the model! Send \"h\" to see the full history]\n",
      "\n",
      "<speaker2> test sa mere\n",
      "--------------------\n",
      "Output of model:\n",
      "a process, and then in paris, then maybe a huge saucer, it is all i love, but that you can <speaker2> you've both finished all to <speaker2> k -'\n",
      " is a \n",
      " can't see a million in my bra, <speaker2>'let's be real pies, it's not much! \n",
      "'i'm nothing.'the blue bottle, and the music with'good ',\n",
      "\n",
      "Input to the model:\n",
      "<speaker1> Yes!\n",
      "<speaker2> Ok let's start chatting\n",
      "<speaker1> Sure, what do you want to talk about?\n",
      "<speaker2> test sa mere\n",
      "<speaker1>\n",
      "--------------------\n",
      "\n",
      "<speaker1> a process, and then in paris, then maybe a huge saucer, it is all i love, but that you can\n",
      "<speaker2> eat my knee\n",
      "--------------------\n",
      "Output of model:\n",
      "that i am in - \" \n",
      " \" i'm going to change my name with a tree? \n",
      " \" the driver, \" she joked, \" are you going to be a total freak, or do you want to go for getting your cats, my boss in the catskills or just to be with your eyebrows. \" \n",
      " \" i can't get your, you should know the last of all these\n",
      "\n",
      "Input to the model:\n",
      "<speaker1> Sure, what do you want to talk about?\n",
      "<speaker2> test sa mere\n",
      "<speaker1> a process, and then in paris, then maybe a huge saucer, it is all i love, but that you can\n",
      "<speaker2> eat my knee\n",
      "<speaker1>\n",
      "--------------------\n",
      "\n",
      "<speaker1> that i am in - \" \n",
      " \" i'm going to change my name with a tree? \n",
      " \" the driver, \" she joked, \" are you going to be a total freak, or do you want to go for getting your cats, my boss in the catskills or just to be with your eyebrows. \" \n",
      " \" i can't get your, you should know the last of all\n",
      "<speaker2> h\n",
      "\n",
      "<speaker2> Hi!\n",
      "<speaker1> Hello\n",
      "<speaker2> Are you ready?\n",
      "<speaker1> Yes!\n",
      "<speaker2> Ok let's start chatting\n",
      "<speaker1> Sure, what do you want to talk about?\n",
      "<speaker2> test sa mere\n",
      "<speaker1> a process, and then in paris, then maybe a huge saucer, it is all i love, but that you can\n",
      "<speaker2> eat my knee\n",
      "<speaker1> that i am in - \" \n",
      " \" i'm going to change my name with a tree? \n",
      " \" the driver, \" she joked, \" are you going to be a total freak, or do you want to go for getting your cats, my boss in the catskills or just to be with your eyebrows. \" \n",
      " \" i can't get your, you should know the last of all\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "speaker1_tag = '<speaker1>'\n",
    "speaker2_tag = '<speaker2>'\n",
    "speaker1_tag_id = tokenizer.convert_tokens_to_ids(speaker1_tag)\n",
    "speaker2_tag_id = tokenizer.convert_tokens_to_ids(speaker2_tag)\n",
    "history = f\"\"\"\n",
    "{speaker2_tag} Hi!\n",
    "{speaker1_tag} Hello\n",
    "{speaker2_tag} Are you ready?\n",
    "{speaker1_tag} Yes!\n",
    "{speaker2_tag} Ok let's start chatting\n",
    "{speaker1_tag} Sure, what do you want to talk about?\"\"\"\n",
    "print(history)\n",
    "print('\\n[Chat with the model! Send \"h\" to see the full history]\\n')\n",
    "history = history.split('\\n')\n",
    "while True: \n",
    "    message = None\n",
    "    while not message:\n",
    "        message = input(f'{speaker2_tag} ')\n",
    "        if message == 'h':\n",
    "            print('\\n'.join(history))\n",
    "            message = None\n",
    "    # add new message to history\n",
    "    history.append(f'{speaker2_tag} {message}')\n",
    "    # keep only most recent conversation as input to the model\n",
    "    recent_history = history[-(2*max_history):]\n",
    "    # concatenate history into single string and add trigger word \"bot:\"\n",
    "    history_str = '{}\\n{}'.format('\\n'.join(recent_history), speaker1_tag)\n",
    "    # tokenize text and convert into vocabulary ids (input ids)\n",
    "    history_enc = tokenizer.encode(history_str, add_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        out_ids = sample_sequence(history_enc, model)\n",
    "    out_ids = out_ids[:, len(history_enc):].tolist()[0]\n",
    "    if not no_info:\n",
    "        print(20*'-')\n",
    "        print('Output of model:')\n",
    "        full_output = tokenizer.decode(out_ids, clean_up_tokenization_spaces=True)\n",
    "        print(full_output)\n",
    "        print('\\nInput to the model:')\n",
    "        print(history_str)\n",
    "        print(20*'-' + '\\n')\n",
    "    # Select part before speaker tags as answer\n",
    "    for i, out_id in enumerate(out_ids):\n",
    "        if out_id in [speaker1_tag_id, speaker2_tag_id]:\n",
    "            break\n",
    "    answer = '{} {}'.format(speaker1_tag, tokenizer.decode(out_ids[:i]))\n",
    "    print(answer)\n",
    "    # add answer to history\n",
    "    history.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FkyH26kEB2Su"
   },
   "source": [
    "You can now review this whole conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNY97GdPB1hn"
   },
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IoqHoD_hW1PW"
   },
   "source": [
    "### Save model to Google Drive\n",
    "If you are happy with your model consider saving it to your Google Drive. Note that all data on this notebook will be lost after a certain time of inactivity. Note that the model size is quite big (~500MB) so make sure you have enough space in your Google Drive.\n",
    "\n",
    "This will save only your final model state (from your directory `run_name` directory).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_NMpXM2W17W"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "#@title Save to Google Drive\n",
    "save_to_drive = False #@param {type:\"boolean\"}\n",
    "\n",
    "source_directory = f'./runs/{run_name}'\n",
    "target_directory = f\"/content/drive/My Drive/AMLD/models/task2/{run_name}/\"\n",
    "include_checkpoints = False\n",
    "\n",
    "if save_to_drive:\n",
    "  logger.info(f'Copying from {source_directory} to {target_directory}...')\n",
    "  ignore_pattern = None\n",
    "  if not include_checkpoints:\n",
    "    ignore_pattern = shutil.ignore_patterns('checkpoint-*')\n",
    "  shutil.copytree(source_directory, target_directory, ignore=ignore_pattern)\n",
    "  logger.info('Successfully copied your model!')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4B6F1B87W6y8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "task-2-artificial-self-AMLD2020.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tb-chat-env': conda)",
   "language": "python",
   "name": "python37764bittbchatenvcondaaf60a6b4a2dc492bb3630ebcc72eea9a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
